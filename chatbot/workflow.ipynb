{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b8cabe0-1222-4f67-a98a-93128196a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import vertexai\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import asyncio\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "#import logging\n",
    "\n",
    "\n",
    "### Define file paths\n",
    "#script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "script_dir = os.getcwd()\n",
    "config_path = os.path.join(script_dir, \"../\", \"config.json\")\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "dotenv_path = os.path.join(script_dir, \"../.env\")\n",
    "#pdf_path = os.path.join(script_dir, \"data\", config[\"pdf_path\"])\n",
    "#output_text_path = os.path.join(script_dir, \"data\", config[\"output_text_path\"])\n",
    "#saved_prompts = os.path.join(script_dir, \"../\", config[\"saved_prompts\"])\n",
    "#log_path = os.path.join(script_dir, \"../\", config[\"logs\"])\n",
    "\n",
    "\n",
    "### Load environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "GOOGLE_CLOUD_PROJECT = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
    "GOOGLE_CLOUD_LOCATION = os.getenv(\"GOOGLE_CLOUD_LOCATION\")\n",
    "GOOGLE_GENAI_USE_VERTEXAI = os.getenv(\"GOOGLE_GENAI_USE_VERTEXAI\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "\n",
    "# # Configure logging\n",
    "# logging.basicConfig(\n",
    "#     filename=log_path,   \n",
    "#     level=logging.INFO,    \n",
    "#     format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "# )\n",
    "\n",
    "\n",
    "### Configure the Language Model\n",
    "vertexai.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_LOCATION) \n",
    "gemini = init_chat_model(\"gemini-2.0-flash-001\", model_provider=\"google_vertexai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f8d5b27-b493-45d2-944a-9da3171f3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed79cdc-fb38-40a7-9ed9-ee3223fc070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "\n",
    "index_name = \"data-vectors\"\n",
    "namespace = \"Acts\"\n",
    "pinecone = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index, namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6fad5cc-c202-493d-a25f-dce50a0b0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = gemini.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "da4f75cb-89a8-42b1-8dc7-c71c29b7dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided context does not specify the maximum number of times a person can be president. The text focuses on the election process, qualifications, and potential removal of a president. Therefore, I cannot answer your question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"How many times person can be president maximum?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f1b9c71-ccab-4175-abbd-5aaa41823a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95ab22-f683-42ef-8d71-a5647a26aefa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
